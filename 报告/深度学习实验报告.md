# 深度学习实验报告

* 李佳政
* 计算技术研究所
* 201828013229075

### 实验二，车牌识别

>给定车牌的三个部分，分别训练三个数据集，分别是汉字，字母，(数字+字母)

#### 基本思路

对图像的像素点由0-255归一化到0-1的区间。随后输入到一个卷积层中，并使用最大池化，并再一次卷积层和赤化层。由于卷积层和池化层都是多维输出，随后将多维数据给平坦化，输出到一个全连接层中作为中间隐藏层，最后是输出层，作为分类概率。对于不同的任务来说，目标输出的类别不同，导致输出层维度不同以外，其它层都是一样的。

#### 具体实现

模型的实现上是比较简单的。使用Keras框架。第一个卷积层的设置如下Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding="same", data_format="channels_last", activation="relu")，最大池化层为MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding="same")。第二个卷计层使用Conv2D(filters=32, kernel_size=(1, 1), strides=(1, 1), padding="same", data_format="channels_last", activation="relu")，池化层MaxPooling2D(pool_size=(1, 1), strides=(1, 1), padding="same")。
由于多维度的卷积结果要输出到全连接层，可以直接使用Flatten层完成平坦化。随后通过两个全连接层Dense(512, activation="relu")和Dropout(rate=0.1)。输出层设置为类别数，激活函数使用softmax，Dense(self.num_classes, activation="softmax")。
在损失函数的选择上使用类别交叉熵损失函数，优化器使用Adam优化器，学习率设置为1e-4，用准确率来评估训练过程效果。

#### 模型summary
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_5 (Conv2D)            (None, 20, 20, 16)        160
_________________________________________________________________
max_pooling2d_5 (MaxPooling2 (None, 10, 10, 16)        0
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 10, 10, 32)        544
_________________________________________________________________
max_pooling2d_6 (MaxPooling2 (None, 10, 10, 32)        0
_________________________________________________________________
flatten_3 (Flatten)          (None, 3200)              0
_________________________________________________________________
dense_5 (Dense)              (None, 512)               1638912
_________________________________________________________________
dropout_3 (Dropout)          (None, 512)               0
_________________________________________________________________
dense_6 (Dense)              (None, 31)                15903
=================================================================
Total params: 1,655,519
Trainable params: 1,655,519
Non-trainable params: 0
```

#### 实验结果
```
Epoch 5/100
5370/5370 [==============================] - 3s 576us/step - loss: 0.0820 - acc: 0.9860
Epoch 6/100
5370/5370 [==============================] - 3s 545us/step - loss: 0.0606 - acc: 0.9896
Epoch 7/100
5370/5370 [==============================] - 3s 606us/step - loss: 0.0491 - acc: 0.9911
Epoch 8/100
5370/5370 [==============================] - 3s 620us/step - loss: 0.0403 - acc: 0.9931
Epoch 9/100
5370/5370 [==============================] - 3s 635us/step - loss: 0.0328 - acc: 0.9940
Epoch 10/100
5370/5370 [==============================] - 3s 567us/step - loss: 0.0275 - acc: 0.9953

test set f1_score is 0.3890493604689382
```

对于简单的字体分类任务来说，数个epoch就可以达到十分高的准确率，但随着训练的继续，虽然训练准确率很少增加，在测试集上的f1评分仍然在增加，如10epoch的评分为38.9，但是到30epoch的情况下，测试集上的结果达到了41分。可以看到，准确率并不是一个合适的评价指标。对于这种简单的ocr识别，图像的预处理等也是比较重要的。由于汉字等比较复杂的场景还没有足够好的准确率。

### 实验三，神经网络语言模型

> 给定PTB数据集，使用LSTM训练语言模型

#### 基本思路

使用RNN对输入的序列建模，输入的数据即是基于词的序列，通过一个embedding层首先转换为稠密的向量，然后输入到LSTM中，每次LSTM计算完之后，会输出一个向量，将这个向量使用softmax层对应到词典中的ID，作为输出。在输出中的设置是每个timestep的输入对应的输出是错位的下一个词，在预处理阶段，为每个词添加开始和结束符号`<s>`与`</s>`，对不足最大长度的句子进行补齐操作。

#### 具体实现

采用Keras实现，使用一层Embedding(VOCAB_SIZE, EMBED_DIM, input_length=MAX_INPUT_LEN)作为输入的转换，然后把输入的词向量传入到LSTM中，使用参数return_sequences=True获取每个time_step的输出，对每个time_step的输出再使用全连接层进行输出，为了参数共享，我们使用TimeDistributed将Dense层包裹住。最后在计算损失函数的时候，使用了类别交叉熵来进行损失优化，并且使用准确率来进行评价语言模型的效果。

#### 实验模型summary
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 30)                0
_________________________________________________________________
embedding_1 (Embedding)      (None, 30, 45)            450090
_________________________________________________________________
lstm_1 (LSTM)                (None, 30, 50)            19200
_________________________________________________________________
time_distributed_1 (TimeDist (None, 30, 10002)         510102
=================================================================
Total params: 979,392
Trainable params: 979,392
Non-trainable params: 0
```

#### 实验结果
```
Epoch 1/10
500/500 [==============================] - 167s 334ms/step - loss: 4.9153 - acc: 0.4058
Epoch 2/10
500/500 [==============================] - 161s 322ms/step - loss: 3.9298 - acc: 0.4227
Epoch 3/10
500/500 [==============================] - 159s 318ms/step - loss: 3.8896 - acc: 0.4342
Epoch 4/10
500/500 [==============================] - 149s 298ms/step - loss: 3.7747 - acc: 0.4432
Epoch 5/10
500/500 [==============================] - 150s 301ms/step - loss: 3.7468 - acc: 0.4528
Epoch 6/10
500/500 [==============================] - 149s 298ms/step - loss: 3.5903 - acc: 0.4674
Epoch 7/10
500/500 [==============================] - 150s 299ms/step - loss: 3.5608 - acc: 0.4714
Epoch 8/10
500/500 [==============================] - 150s 300ms/step - loss: 3.4401 - acc: 0.4909
Epoch 9/10
500/500 [==============================] - 149s 299ms/step - loss: 3.4579 - acc: 0.4892
Epoch 10/10
500/500 [==============================] - 150s 300ms/step - loss: 3.3709 - acc: 0.5002
```

可以看到随着epoch的增加，实验的准确率是不断增加的，但是准确率始终在50左右徘徊，我认为主要原因是在LSTM中，存在冷启动的原因，例如`<s>`后的可能词语是有很多的，随着确定的单词逐渐增加，那么对应的可能单词数也在不断减少，即确定的条件概率不断增加，从而获得更高的准确率。可以看到loss是在不断下降的，但是下降的很慢，主要原因可能也是受到上述问题的影响。相比word2vec单层，LSTM对长序列的输入效果确实更好，如果能使用一些word2vec的技巧，对这个语言模型的改进应当还会有所帮助，但主流的特征抽取可能都转向了transformer，由于其良好的并行性和优越的性能。

### 实验四，文本分类

>给定一个句子，进行情感二分类。

#### 基本思路

本次实验要求使用textCNN模型，是一个基于CNN的文本分类模型，由于在之前图像实验上有过使用CNN的经验，所以实验模型的搭建是比较简单的。首先将句子的但词都转换为word embedding，将每个词向量的维度视为宽度，由于缺少channel，所以我们使用Conv1D来代替图像中的2D卷积。输入到CNN中后，我们获得了一个向量转换，随后经过多个全连接层后，经过softmax全连接层得到两个概率值，或者一个sigmoid全连接层来输出对应情感的概率。根据输出的形式不同，采用不同的target。

#### 具体实现

首先我们需要定义输入Input(shape=(MAX_LEN, ), dtype="int32")，也就是输入的句子，设定最大长度，不足的进行不全。随后我们使用Embedding(len(word_to_id), embed_mat.shape[-1], weights=[embed_mat],  input_length=MAX_LEN, trainable=False, name="embed",)来进行词向量的转换，与上个实验不同的是，我们根据给定的glove词向量，而不必重新训练词向量。然后我们把转换好的句子词向量输入到卷积网络中，Conv1D(filters=20, kernel_size=3, strides=1, padding="same", activation="relu")(input_emb)。经过全局最大池化后,
```
def reduce_max(conv):
    return tf.reduce_max(conv, reduction_indices=[1], name="gmp")
gmp = Lambda(reduce_max)(conv)
```
再输入到两个全连接网络中Dense(50, name="fc1", activation="relu")(gmp)，Dense(2, activation="softmax")(hidden)。
在损失函数上，我们依旧使用类别交叉熵损失，使用准确率来检验模型的性能。

#### 实验模型summary
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 70)                0
_________________________________________________________________
embed (Embedding)            (None, 70, 50)            2947700
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 70, 20)            3020
_________________________________________________________________
lambda_1 (Lambda)            (None, 20)                0
_________________________________________________________________
dropout_1 (Dropout)          (None, 20)                0
_________________________________________________________________
fc1 (Dense)                  (None, 50)                1050
_________________________________________________________________
dropout_2 (Dropout)          (None, 50)                0
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 102
=================================================================
Total params: 2,951,872
Trainable params: 4,172
Non-trainable params: 2,947,700
```

#### 实验结果
```
Epoch 1/100
19998/19998 [==============================] - 4s 187us/step - loss: 0.6415 - acc: 0.6273 - val_loss: 0.5306 - val_acc: 0.7433
Epoch 2/100
19998/19998 [==============================] - 3s 150us/step - loss: 0.5488 - acc: 0.7264 - val_loss: 0.4874 - val_acc: 0.7727
Epoch 3/100
19998/19998 [==============================] - 3s 145us/step - loss: 0.5152 - acc: 0.7476 - val_loss: 0.4781 - val_acc: 0.7753
Epoch 4/100
19998/19998 [==============================] - 3s 148us/step - loss: 0.4912 - acc: 0.7627 - val_loss: 0.4472 - val_acc: 0.7948
Epoch 5/100
19998/19998 [==============================] - 3s 145us/step - loss: 0.4765 - acc: 0.7746 - val_loss: 0.4317 - val_acc: 0.8066

... ...

Epoch 96/100
19998/19998 [==============================] - 4s 179us/step - loss: 0.3578 - acc: 0.8413 - val_loss: 0.2944 - val_acc: 0.8829
Epoch 97/100
19998/19998 [==============================] - 3s 174us/step - loss: 0.3609 - acc: 0.8374 - val_loss: 0.3099 - val_acc: 0.8722
Epoch 98/100
19998/19998 [==============================] - 4s 183us/step - loss: 0.3524 - acc: 0.8440 - val_loss: 0.2944 - val_acc: 0.8805
Epoch 99/100
19998/19998 [==============================] - 3s 169us/step - loss: 0.3614 - acc: 0.8381 - val_loss: 0.2959 - val_acc: 0.8809
Epoch 100/100
19998/19998 [==============================] - 4s 176us/step - loss: 0.3573 - acc: 0.8410 - val_loss: 0.2930 - val_acc: 0.8792
```

可以看出，随着训练epoch的不断增加，验证集的损失在不断下降，准确率从74提升到到88，可以看出并没有到达过拟合的epoch次数，只是受限于模型的能力。由于CNN没有对时序数据的处理能力，所以对词语组合上没有太好的预测效果。


### 实验五，猫狗分类

>使用迁移学习方法，进行分类，而不是重新训练全部网络权重

#### 实验思路

调用tensorflow的slim封装接口，这个框架内包含了一些经典的神经网络框架的实现，所以我们可以通过这个接口直接调用模型，通过预训练的模型，可以直接对图像进行抽取特征，而不需要重新训练。由于猫狗分类任务中与Inception原始分类任务的预测种类不同，所以我们固定住前面的权重，而只通过训练样本来训练最后一层Logtis.减少更新的权重数量。

#### 具体操作

1. 检测slim安装，并下载模型
```
python -c "import tensorflow.contrib.slim as slim; eval = slim.evaluation.evaluate_once"
git clone https://github.com/tensorflow/models/
```

2. 修改代码，转换数据，在dogs文件夹中，已经准备好了分类文件，以类别名称分类文件，`./dogs/cats/*.jpg`和`./dogs/dogs/*.jpg`。
```python
# 增加download_and_convert_dogs.py，复制download_and_convert_flowers.py
_NUM_VALIDATION = 180
_NUM_SHARDS = 2

# 修改download_and_convert_data.py
from datasets import download_and_convert_dogs
def main(_):
    ...
    elif FLAGS.dataset_name == "dogs":
        download_and_convert_dogs.run(FLAGS.dataset_dir)
    ...

# 运行python脚本
python download_and_convert_data.py --dataset_name=dogs --dataset_dir=./
```

3. 训练模型
```python
# 增加dogVScats.py
_FILE_PATTERN = 'dogs_%s_*.tfrecord'
SPLITS_TO_SIZES = {'train': 548, 'validation': 180}
_NUM_CLASSES = 2

# 修改dataset_factory.py
from datasets import dogsVScats
datasets_map = {
    'cifar10': cifar10,
    'flowers': flowers,
    'imagenet': imagenet,
    'mnist': mnist,
    'visualwakewords': visualwakewords,
    'dogsVScats': dogsVScats,
  }
```

* 训练命令
```shell
python train_image_classifier.py \
    --train_dir=dogsVScats/train_dir \
    --dataset_name=dogsVScats \
    --dataset_split_name=train \
    --dataset_dir=dogsVScats/data \
    --model_name=inception_v3 \
    --checkpoint_path=dogsVScats/pretrained/inception_v3.ckpt \
    --checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \
    --trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \
    --max_number_of_steps=100 \
    --batch_size=32 \
    --learning_rate=0.001 \
    --learning_rate_decay_type=fixed \
    --save_interval_secs=300 \
    --save_summaries_secs=2 \
    --log_every_n_steps=1 \
    --optimizer=rmsprop \
    --weight_decay=0.00004

# 使用CPU训练时，将参数--clone_on_cpu设置为True
```

* 评估命令
```shell
python eval_image_classifier.py \
    --checkpoint_path=dogsVScats/train_dir \
    --eval_dir=dogsVScats/eval_dir \
    --dataset_name=dogsVScats \
    --dataset_split_name=validation \
    --dataset_dir=dogsVScats/data \
    --model_name=inception_v3
```

#### 实验结果
* 训练过程
```shell

INFO:tensorflow:global step 1: loss = 1.2904 (66.466 sec/step)
INFO:tensorflow:Recording summary at step 1.
INFO:tensorflow:global step 2: loss = 1.1535 (52.738 sec/step)
INFO:tensorflow:Recording summary at step 2.
INFO:tensorflow:global step 3: loss = 1.1574 (52.292 sec/step)
INFO:tensorflow:Recording summary at step 3.
INFO:tensorflow:global step 4: loss = 1.1693 (47.200 sec/step)
INFO:tensorflow:Recording summary at step 4.
INFO:tensorflow:global step 5: loss = 1.2112 (49.057 sec/step)
INFO:tensorflow:Saving checkpoint to path dogsVScats/train_dir/model.ckpt
INFO:tensorflow:Recording summary at step 5.
INFO:tensorflow:global step 6: loss = 1.2455 (53.392 sec/step)

...

INFO:tensorflow:global step 97: loss = 0.7113 (18.106 sec/step)
INFO:tensorflow:Saving checkpoint to path dogsVScats/train_dir/model.ckpt
INFO:tensorflow:Recording summary at step 97.
INFO:tensorflow:global step 98: loss = 0.6688 (34.582 sec/step)
INFO:tensorflow:global step 99: loss = 0.8462 (45.917 sec/step)
INFO:tensorflow:Recording summary at step 99.
INFO:tensorflow:global step 100: loss = 0.6936 (43.762 sec/step)
INFO:tensorflow:Stopping Training.
INFO:tensorflow:Finished training! Saving model to disk.
INFO:tensorflow:Recording summary at step 100.
```

* 评估过程
```
INFO:tensorflow:Evaluation [1/2]
INFO:tensorflow:Evaluation [2/2]
eval/Recall_5[1]
eval/Accuracy[0.995]
```

* 实验分析
通过上述流程后，我们很快可以获得一个准确度特别高的图像分类系统，所以迁移学习在实际工程应用中是十分有用的，可以省去大量的计算资源来达到相同的效果。并且无需使用大量的训练样本，就可以达到令人满意的效果。
本次实验通过实战学会了如何使用迁移学习这一大深度学习利器，在今后使用深度学习这个工具的时候，能够更加运用自如。


### 语音识别

> 给定一段英文数字的语音，训练模型后分类

#### 基本思路

语音与图像不同，需要采用的特征转换方式比较复杂，通过librosa对输入的语音进行转换，得到mfcc频谱特征，包含加窗、权重等一系列操作，转换后的数据作为神经网络的输入。输入后通过多个卷积层，最后通过一个全连接层和softmax输出进行分类，损失函数也是多分类情况下的损失函数

#### 具体实现

实现上采用tensorflow框架。tf.layers.conv1d作为卷积接口，将多个padding后的结果concat起来后通过dense全连接层输出到隐藏层，然后再通过输出层，输出多个类别的概率。损失函数使用tf.nn.softmax_cross_entropy_with_logits。之后通过tf.reduce_mean将一个batch内所有样本损失求平均。在计算准确率的时候，使用tf.reduce_mean和tf.equal求准确率。
提取特征的时候使用librosa.feature.mfcc接口。
使用Adam优化。


#### 实验结果

使用全量数据集，十个epoch进行验证。

```
Epoch: 1
Steps: 10
train_loss: 2.215318441390991,     train_accuracy: 0.23863635957241058
valid_loss: 2.4143941402435303,     valid_accuracy: 0.12965340912342072
Steps: 20
train_loss: 1.9598276615142822,     train_accuracy: 0.3589743673801422
valid_loss: 2.2206778526306152,     valid_accuracy: 0.19512194395065308
Steps: 30
train_loss: 1.7626045942306519,     train_accuracy: 0.529411792755127
valid_loss: 2.12593150138855,     valid_accuracy: 0.24005134403705597
Steps: 40
train_loss: 1.621239423751831,     train_accuracy: 0.5344827771186829
valid_loss: 2.0708210468292236,     valid_accuracy: 0.2657252848148346
Steps: 50
train_loss: 1.186545729637146,     train_accuracy: 0.7916666865348816
valid_loss: 2.1386165618896484,     valid_accuracy: 0.26059049367904663
Steps: 60
train_loss: 1.092586636543274,     train_accuracy: 0.7631579041481018
valid_loss: 2.116523265838623,     valid_accuracy: 0.2811296582221985
Steps: 70
train_loss: 0.8035584092140198,     train_accuracy: 0.8928571343421936
valid_loss: 2.153146266937256,     valid_accuracy: 0.3068035840988159
Steps: 80
train_loss: 0.6942559480667114,     train_accuracy: 0.9444444179534912
valid_loss: 2.430180549621582,     valid_accuracy: 0.27214378118515015
Steps: 90
train_loss: 0.32504749298095703,     train_accuracy: 1.0
valid_loss: 3.127337694168091,     valid_accuracy: 0.2336328625679016
test_loss is 4.049326419830322,      test_accuracy is 0.1111111119389534

... ...


Epoch: 10
Steps: 880
train_loss: 0.05257471278309822,     train_accuracy: 0.9890109896659851
valid_loss: 1.9970823526382446,     valid_accuracy: 0.5198972821235657
Steps: 890
train_loss: 0.04066906124353409,     train_accuracy: 0.9876543283462524
valid_loss: 1.733708381652832,     valid_accuracy: 0.5468549132347107
Steps: 900
train_loss: 0.018603337928652763,     train_accuracy: 1.0
valid_loss: 1.6621384620666504,     valid_accuracy: 0.5596919059753418
Steps: 910
train_loss: 0.031886469572782516,     train_accuracy: 1.0
valid_loss: 1.6070492267608643,     valid_accuracy: 0.5558408498764038
Steps: 920
train_loss: 0.03574521839618683,     train_accuracy: 1.0
valid_loss: 1.729973316192627,     valid_accuracy: 0.5532734394073486
Steps: 930
train_loss: 0.014563658274710178,     train_accuracy: 1.0
valid_loss: 1.6830357313156128,     valid_accuracy: 0.562259316444397
Steps: 940
train_loss: 0.018795374780893326,     train_accuracy: 1.0
valid_loss: 1.7676584720611572,     valid_accuracy: 0.5391527414321899
Steps: 950
train_loss: 0.022695962339639664,     train_accuracy: 1.0
valid_loss: 1.7687444686889648,     valid_accuracy: 0.5558408498764038
Steps: 960
train_loss: 0.008410765789449215,     train_accuracy: 1.0
valid_loss: 1.7099878787994385,     valid_accuracy: 0.5442875623703003
Steps: 970
train_loss: 0.00028939827461726964,     train_accuracy: 1.0
valid_loss: 2.147282838821411,     valid_accuracy: 0.4967907667160034
test_loss is 1.8142822980880737,      test_accuracy is 0.4444444477558136
```

可以看出，随着训练的不断深入，大概５个epoch的时候，训练集的准确率就已经不变了，恒定为１，但是损失仍然在下降。验证集的准确率也是在不断上升，但是波动也比较明显。通过调整训练集的大小时发现，小训练集(shuffle后)的效果十分不好，容易产生过拟合，可能验证集和训练集的声音特征上还是有一些分布上的差异，导致验证集的准确率无法提升很高。
调整卷积filter_size为逐渐递减后，对准确率的影响也比较大，尽可能地保留更多的信息，会增加全连接层的参数，但是会提高准确率。
